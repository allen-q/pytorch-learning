{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/trump_speeches.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/names.csv')\n",
    "data = ' '.join(df.name.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 237097 characters, 53 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(data), len(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_id = np.array([chars.index(c) for c in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[np.arange(len(X_train)), char_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.roll(char_id,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237097, 53)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237097,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc_x = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, X, h):\n",
    "        x_out = self.fc_x(X)\n",
    "        h_out = self.fc_h(h)\n",
    "        h_new = self.tanh(x_out + h_out)\n",
    "        out = self.fc_out(h_new)\n",
    "        \n",
    "        return out, h_new\n",
    "    \n",
    "    def init_h(self):\n",
    "        return torch.zeros(self.fc_x.out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rnn(torch.from_numpy(X_train[0]).float(), torch.zeros(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = CharRNN(vocab_size, hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X=X_train, y=y_train, batch_size=seq_length):\n",
    "    X = torch.from_numpy(X).float()\n",
    "    y = torch.from_numpy(y).long()\n",
    "    for i in range(0, len(y), batch_size):   \n",
    "        id_stop = i+batch_size if i+batch_size < len(X) else len(X)\n",
    "        yield([X[i:id_stop], y[i:id_stop]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_chars(X_seed, h_prev, length=20):\n",
    "    for p in rnn.parameters():\n",
    "        p.requires_grad = False\n",
    "    X_next = X_seed\n",
    "    results = []\n",
    "    for i in range(length):        \n",
    "        y_score, h_prev = rnn(X_next, h_prev)\n",
    "        y_prob = nn.Softmax(0)(y_score).detach().numpy()\n",
    "        y_pred = np.random.choice(chars,1, p=y_prob).item()\n",
    "        results.append(y_pred)\n",
    "        X_next = torch.zeros_like(X_seed)\n",
    "        X_next[chars.index(y_pred)] = 1\n",
    "        #print(f'{i} th char:{y_pred}')\n",
    "    for p in rnn.parameters():\n",
    "        p.requires_grad = True\n",
    "    return ''.join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss:80.22566986083984 at iter: 1000\n",
      "Batch Loss:74.82136535644531 at iter: 2000\n",
      "Batch Loss:83.30944061279297 at iter: 3000\n",
      "Batch Loss:69.32365417480469 at iter: 4000\n",
      "Batch Loss:73.34046173095703 at iter: 5000\n",
      "Batch Loss:70.41938781738281 at iter: 6000\n",
      "Batch Loss:71.80207824707031 at iter: 7000\n",
      "Batch Loss:60.49985885620117 at iter: 8000\n",
      "Batch Loss:66.22582244873047 at iter: 9000\n",
      "Running Avg Loss:2.815822776594758 at epoch: 0\n",
      "rinede Bnan Aeiaey Coyiee Karirt aama Mclra taalae LdrreeVMwJnayi aqyyh Biyy Maataqy Bmaycaljod CmoiieG Panlana Rliy aeMlclnea Mliia Aucoiov zaieim trnvyiAiSiili Miysye Ryowa Ehiuan Jodl nynenet Calay\n",
      "Batch Loss:57.404273986816406 at iter: 10000\n",
      "Batch Loss:64.58155822753906 at iter: 11000\n",
      "Batch Loss:71.63925170898438 at iter: 12000\n",
      "Batch Loss:60.29502487182617 at iter: 13000\n",
      "Batch Loss:69.5533447265625 at iter: 14000\n",
      "Batch Loss:55.31587600708008 at iter: 15000\n",
      "Batch Loss:65.75656127929688 at iter: 16000\n",
      "Batch Loss:51.510169982910156 at iter: 17000\n",
      "Batch Loss:61.49117660522461 at iter: 18000\n",
      "Batch Loss:59.18311309814453 at iter: 19000\n",
      "Running Avg Loss:2.390102558992803 at epoch: 0\n",
      "an mnaslee Karzera Lmren Tealee Teledginn Earin Jeesa Dahey Eaeiis Treran Eoigh Anrlyy haMa Alalae Siriela SrkyanAh Celyna Aboleie Anayne Esarei Eilaelo Maiil Lyyon Runon SehvMvyn JnaernahBheyl Blarda\n",
      "Batch Loss:57.53322219848633 at iter: 20000\n",
      "Batch Loss:59.818111419677734 at iter: 21000\n",
      "Batch Loss:57.99853515625 at iter: 22000\n",
      "Batch Loss:56.70036315917969 at iter: 23000\n",
      "Batch Loss:65.86338806152344 at iter: 24000\n",
      "Batch Loss:61.35068130493164 at iter: 25000\n",
      "Batch Loss:56.08069610595703 at iter: 26000\n",
      "Batch Loss:57.87437057495117 at iter: 27000\n",
      "Batch Loss:59.46806716918945 at iter: 28000\n",
      "Batch Loss:54.299659729003906 at iter: 29000\n",
      "Running Avg Loss:2.260102296924591 at epoch: 0\n",
      "a Kamkilen Belered Sllstaia Alugmv Eolena KaeliRt Pyelli Alialiih Manroa Surlanne Lmerlyn Iyyne Selbyna Arenen Mahyna Jovaadaa Smelmne Ldebily Lannle Keryana Eamcelees Jtbmillia Kebolyn Lrslalniyy Cra\n",
      "Batch Loss:48.46607971191406 at iter: 30000\n",
      "Batch Loss:57.718257904052734 at iter: 31000\n",
      "Batch Loss:52.499786376953125 at iter: 32000\n",
      "Batch Loss:53.83773422241211 at iter: 33000\n",
      "Batch Loss:54.499576568603516 at iter: 34000\n",
      "Batch Loss:63.10764694213867 at iter: 35000\n",
      "Batch Loss:48.10704803466797 at iter: 36000\n",
      "Batch Loss:58.08310317993164 at iter: 37000\n",
      "Batch Loss:55.99266815185547 at iter: 38000\n",
      "Batch Loss:52.39011764526367 at iter: 39000\n",
      "Running Avg Loss:2.252966988090053 at epoch: 0\n",
      " Kiagi Lian Ieycl Yevia Larie Marerra Nyanne Haydan Kebyne Monsya Siel Murerosa Coeola Atkerdri Aloniwr CKia Marole Laliaa Mrryna Ma Narl Anezse Sance MionaDua Jeynn Sysla Kauija Mane Jonn Ila Ohla Av\n",
      "Batch Loss:60.32847595214844 at iter: 40000\n",
      "Batch Loss:65.09903717041016 at iter: 41000\n",
      "Batch Loss:60.01361083984375 at iter: 42000\n",
      "Batch Loss:60.62772750854492 at iter: 43000\n",
      "Batch Loss:49.027313232421875 at iter: 44000\n",
      "Batch Loss:68.7728500366211 at iter: 45000\n",
      "Batch Loss:50.934539794921875 at iter: 46000\n",
      "Batch Loss:56.370140075683594 at iter: 47000\n",
      "Batch Loss:59.997920989990234 at iter: 48000\n",
      "Batch Loss:52.5783576965332 at iter: 49000\n",
      "Running Avg Loss:2.219937785959989 at epoch: 0\n",
      "las Oelena Gianah Kallya Jarsdrongh Kanise zamka Davana Yroos Calata Raruia Dilynnjataf Cail Zdeyma Zeilhy Cidri Hrisllah Hamiyna TalWish Saivoa Bapvaina Caliah Mevlin Terisha LiekllikeGlondellein uhl\n",
      "Batch Loss:63.275726318359375 at iter: 50000\n",
      "Batch Loss:53.770103454589844 at iter: 51000\n",
      "Batch Loss:37.775550842285156 at iter: 52000\n",
      "Batch Loss:50.10419464111328 at iter: 53000\n",
      "Batch Loss:57.83484649658203 at iter: 54000\n",
      "Batch Loss:51.546592712402344 at iter: 55000\n",
      "Batch Loss:51.29753875732422 at iter: 56000\n",
      "Batch Loss:55.9918098449707 at iter: 57000\n",
      "Batch Loss:57.813899993896484 at iter: 58000\n",
      "Batch Loss:55.45759582519531 at iter: 59000\n",
      "Running Avg Loss:2.220391572130844 at epoch: 0\n",
      " Nivista Jatulei Mepdri Cattena Huxa Niah Vealia Lupea Lexsalbe Mecinh Kamay Keliyaha Lelyy K yra Liah Avizne Farea Megheri Khah Jukite Jyke Marie Yoraala NiuVayna Sennyne Vokyia Mildina Anikarre Trie\n",
      "Batch Loss:53.19626998901367 at iter: 60000\n",
      "Batch Loss:49.15373229980469 at iter: 61000\n",
      "Batch Loss:54.82253646850586 at iter: 62000\n",
      "Batch Loss:51.857017517089844 at iter: 63000\n",
      "Batch Loss:54.73291015625 at iter: 64000\n",
      "Batch Loss:52.2182502746582 at iter: 65000\n",
      "Batch Loss:47.23321533203125 at iter: 66000\n",
      "Batch Loss:47.29219055175781 at iter: 67000\n",
      "Batch Loss:51.710227966308594 at iter: 68000\n",
      "Batch Loss:49.325443267822266 at iter: 69000\n",
      "Running Avg Loss:2.186884402401 at epoch: 0\n",
      "i Cana Mama BrasriQqfi Breylon Jamtna Datanneen Koda Aslia Brarlynn Zoylann Elazis Nelleki Derlin Yanelce Knarini Jafelen MatUsan Chiden Horeelen Arira Cliya Curya Jalenna Bruslynn Kancee Tnase Chansl\n",
      "Batch Loss:47.35625076293945 at iter: 70000\n",
      "Batch Loss:49.67082595825195 at iter: 71000\n",
      "Batch Loss:64.9109115600586 at iter: 72000\n",
      "Batch Loss:42.506927490234375 at iter: 73000\n",
      "Batch Loss:45.048858642578125 at iter: 74000\n",
      "Batch Loss:49.78304672241211 at iter: 75000\n",
      "Batch Loss:60.06364822387695 at iter: 76000\n",
      "Batch Loss:64.01777648925781 at iter: 77000\n",
      "Batch Loss:52.891727447509766 at iter: 78000\n",
      "Batch Loss:39.76060485839844 at iter: 79000\n",
      "Running Avg Loss:2.172075716534257 at epoch: 0\n",
      "un Zabe Bredina LeesInt Debya Annnya Sainrze Xoma geyah Auezor BNesta Bmyah Anaryy h elineS Eplinah Reaiala Svitna Chend Emyan Abritna Arselia Marima Aixban Banala Breini Zeran Alyza Amcria Zonarich D\n",
      "Batch Loss:48.39973068237305 at iter: 80000\n",
      "Batch Loss:53.68559646606445 at iter: 81000\n",
      "Batch Loss:58.26463317871094 at iter: 82000\n",
      "Batch Loss:53.35237121582031 at iter: 83000\n",
      "Batch Loss:62.593509674072266 at iter: 84000\n",
      "Batch Loss:50.52855682373047 at iter: 85000\n",
      "Batch Loss:45.35763931274414 at iter: 86000\n",
      "Batch Loss:71.63772583007812 at iter: 87000\n",
      "Batch Loss:47.18706130981445 at iter: 88000\n",
      "Batch Loss:45.90482711791992 at iter: 89000\n",
      "Running Avg Loss:2.1359447175020354 at epoch: 0\n",
      "Mrdellyn Karra Fathy hannoVi Junge Joteiza Kcirin GisazSa Mabyn Jothee Ka Tynde Jorase Kaiejah kolie Jahandel Haeilynn Kannie Ikkrisa Rubele RuveimaPEvka muusn Deiia Nahyly Naelya Kazieh Eele Jaziee P\n",
      "Batch Loss:45.91722106933594 at iter: 90000\n",
      "Batch Loss:46.694461822509766 at iter: 91000\n",
      "Batch Loss:59.98139953613281 at iter: 92000\n",
      "Batch Loss:57.15916442871094 at iter: 93000\n",
      "Batch Loss:55.30760955810547 at iter: 94000\n",
      "Batch Loss:42.930171966552734 at iter: 95000\n",
      "Batch Loss:51.469703674316406 at iter: 96000\n",
      "Batch Loss:47.87586212158203 at iter: 97000\n",
      "Batch Loss:56.27677536010742 at iter: 98000\n",
      "Batch Loss:51.27521514892578 at iter: 99000\n",
      "Running Avg Loss:2.0974962736766787 at epoch: 0\n",
      " Kharyah Kaselai Kathee Keziele Kiolie Kerya Kerase JuTari Knei qislyng Kaynni Kaily Heyle Cunriah Karennn Kinnoni Kotoley Kianly Harele Karleen Kecin Haysay Kemily Khyli Xancinn KZamaea Kelyn Kuerie \n",
      "Batch Loss:50.864986419677734 at iter: 100000\n",
      "Batch Loss:48.58708953857422 at iter: 101000\n",
      "Batch Loss:54.93387222290039 at iter: 102000\n",
      "Batch Loss:51.14816665649414 at iter: 103000\n",
      "Batch Loss:56.52349853515625 at iter: 104000\n",
      "Batch Loss:76.91436004638672 at iter: 105000\n",
      "Batch Loss:37.77809524536133 at iter: 106000\n",
      "Batch Loss:45.32442855834961 at iter: 107000\n",
      "Batch Loss:50.631343841552734 at iter: 108000\n",
      "Batch Loss:45.79193115234375 at iter: 109000\n",
      "Running Avg Loss:2.109506176293269 at epoch: 0\n",
      "vazya Emilia Emcez Eslesre Evlin Cina Esdrrye Frarria Emiste EvaliGa Cylin Ellle Eylyl xreina Kizrah Gile Ennanna Rami Evanny Ellinn Eriyn Keelyr Jeyh Eviy Kymmina Evana Ellia Emie EScini Evdymar Eran\n",
      "Batch Loss:53.97742462158203 at iter: 110000\n",
      "Batch Loss:57.162513732910156 at iter: 111000\n",
      "Batch Loss:50.485660552978516 at iter: 112000\n",
      "Batch Loss:42.481422424316406 at iter: 113000\n",
      "Batch Loss:38.91913986206055 at iter: 114000\n",
      "Batch Loss:48.73381042480469 at iter: 115000\n",
      "Batch Loss:61.5672607421875 at iter: 116000\n",
      "Batch Loss:58.3242301940918 at iter: 117000\n",
      "Batch Loss:55.8750114440918 at iter: 118000\n",
      "Batch Loss:50.892704010009766 at iter: 119000\n",
      "Running Avg Loss:2.1216771037574857 at epoch: 0\n",
      "ailisnyda Avikui Bhaisa AAnjah ABragen Vicea AbbeleeZ Brinndi Aleiabelibchani Aiahha AAthubelaAnelphishya Yareryazinaras Aelin Zarimina Evian Ahjamamiish Andyah AEmyinA Atophalyannea Avela Aiisha Zary\n",
      "Batch Loss:52.15435791015625 at iter: 120000\n",
      "Batch Loss:47.16081237792969 at iter: 121000\n",
      "Batch Loss:51.101749420166016 at iter: 122000\n",
      "Batch Loss:51.53009033203125 at iter: 123000\n",
      "Batch Loss:47.2504768371582 at iter: 124000\n",
      "Batch Loss:52.7958984375 at iter: 125000\n",
      "Batch Loss:63.750030517578125 at iter: 126000\n",
      "Batch Loss:62.039791107177734 at iter: 127000\n",
      "Batch Loss:43.06178283691406 at iter: 128000\n",
      "Batch Loss:54.16642761230469 at iter: 129000\n",
      "Running Avg Loss:2.0281031943200154 at epoch: 0\n",
      "ey Keizka Kemcavik Kunsya Karfh ivenn Ktimy Eme Imilee Kaambollea Leenci Kanilen Keimarni Kakeleah Keoliri KPessa Kalyni Kari Kaylan Kiamdy h Kairee Kazrile Kabrih Kandya Kelenit Keabin Kendilaha Krur\n",
      "Batch Loss:52.54957580566406 at iter: 130000\n",
      "Batch Loss:38.22495651245117 at iter: 131000\n",
      "Batch Loss:49.850379943847656 at iter: 132000\n",
      "Batch Loss:46.704349517822266 at iter: 133000\n",
      "Batch Loss:44.147193908691406 at iter: 134000\n",
      "Batch Loss:56.66978454589844 at iter: 135000\n",
      "Batch Loss:62.2896614074707 at iter: 136000\n",
      "Batch Loss:72.32203674316406 at iter: 137000\n",
      "Batch Loss:59.12324142456055 at iter: 138000\n",
      "Batch Loss:49.28346252441406 at iter: 139000\n",
      "Running Avg Loss:2.115974710586667 at epoch: 0\n",
      "en Kiaven Yobrea Zavlana Aurweyl Cdrialle Chaysha Haslen Esiyaha Zgureenn Vileysh Lilas yanne maxLeym Brisma Nakandannu Griaran Kinni MasbeFa Sarzyn Ilevimah Nellel Kedtwal Niovee Jatary Mayra Falalah\n",
      "Batch Loss:58.10580825805664 at iter: 140000\n",
      "Batch Loss:52.49660110473633 at iter: 141000\n",
      "Batch Loss:59.54730224609375 at iter: 142000\n",
      "Batch Loss:51.67321014404297 at iter: 143000\n",
      "Batch Loss:63.25729751586914 at iter: 144000\n",
      "Batch Loss:60.99144744873047 at iter: 145000\n",
      "Batch Loss:59.538394927978516 at iter: 146000\n",
      "Batch Loss:51.33732223510742 at iter: 147000\n",
      "Batch Loss:64.45069122314453 at iter: 148000\n",
      "Batch Loss:62.46648025512695 at iter: 149000\n",
      "Running Avg Loss:2.3031766728824006 at epoch: 0\n",
      " Kiosh Kerva Timor Kaon Yinden Laylen KaslZison Nod Miy Donnie Driameriss Maidon Jolynn Zanuilin Malia Bravi Gan Suvaev NaaKijlen Namcen Junaon Prenadlad Bristan Jaklon Trhall miyqLiian Adeen Jemeuzmi\n",
      "Batch Loss:56.97519302368164 at iter: 150000\n",
      "Batch Loss:57.252906799316406 at iter: 151000\n",
      "Batch Loss:55.759525299072266 at iter: 152000\n",
      "Batch Loss:62.01871871948242 at iter: 153000\n",
      "Batch Loss:58.53398895263672 at iter: 154000\n",
      "Batch Loss:70.22566986083984 at iter: 155000\n",
      "Batch Loss:61.95641326904297 at iter: 156000\n",
      "Batch Loss:65.25480651855469 at iter: 157000\n",
      "Batch Loss:61.578025817871094 at iter: 158000\n",
      "Batch Loss:56.85655212402344 at iter: 159000\n",
      "Running Avg Loss:2.2734679769428445 at epoch: 0\n",
      "dash Mickeer Jtyden Jaym Myldnndond Idraj Terre Lusinno zeriyah CJentman Maynon Khan Liigtah uorur Arryar Tries Fayron AriJy OmonaSy Maurson Daom yny Giollio AleRyn Jarick Giynde Shannor Abamabiten Az\n",
      "Batch Loss:52.987239837646484 at iter: 160000\n",
      "Batch Loss:53.93489074707031 at iter: 161000\n",
      "Batch Loss:52.60896682739258 at iter: 162000\n",
      "Batch Loss:61.61288833618164 at iter: 163000\n",
      "Batch Loss:62.750755310058594 at iter: 164000\n",
      "Batch Loss:55.60999298095703 at iter: 165000\n",
      "Batch Loss:61.24724578857422 at iter: 166000\n",
      "Batch Loss:47.74062728881836 at iter: 167000\n",
      "Batch Loss:55.538047790527344 at iter: 168000\n",
      "Batch Loss:69.03003692626953 at iter: 169000\n",
      "Running Avg Loss:2.2831035456625743 at epoch: 0\n",
      " Raylin Kaydin Myvonde Jonin Len Anzeeto Terucu Ryshel Madimo Hestellig Evi Watyne Navi Cgosteel Sephouk Maikore Padyl Uson Cortel Maimionn Yalamim Marmis Juwtios Bersynix Gastilko Rajariy Amriy Kyne \n",
      "Batch Loss:63.281375885009766 at iter: 170000\n",
      "Batch Loss:62.67234802246094 at iter: 171000\n",
      "Batch Loss:53.16225051879883 at iter: 172000\n",
      "Batch Loss:61.726627349853516 at iter: 173000\n",
      "Batch Loss:51.06203079223633 at iter: 174000\n",
      "Batch Loss:58.291954040527344 at iter: 175000\n",
      "Batch Loss:47.1977424621582 at iter: 176000\n",
      "Batch Loss:57.64182662963867 at iter: 177000\n",
      "Batch Loss:63.119056701660156 at iter: 178000\n",
      "Batch Loss:57.891082763671875 at iter: 179000\n",
      "Running Avg Loss:2.31753959070649 at epoch: 0\n",
      "dan Klybrmon Ily Kadv Jar Kosha Joresh Lynathas Brundego Khorgey Javonak Joun Evon Jeiva Dakesn Olary Nickoe Demon Joste Damoo Caydens Mikzaw Che Mizer Kanah Joldik Cahmiry Madt Miwy Dakhtr Teopan Kar\n",
      "Batch Loss:58.175716400146484 at iter: 180000\n",
      "Batch Loss:62.11664962768555 at iter: 181000\n",
      "Batch Loss:69.7607421875 at iter: 182000\n",
      "Batch Loss:58.51458740234375 at iter: 183000\n",
      "Batch Loss:48.4360237121582 at iter: 184000\n",
      "Batch Loss:52.26556396484375 at iter: 185000\n",
      "Batch Loss:63.39214324951172 at iter: 186000\n",
      "Batch Loss:59.142024993896484 at iter: 187000\n",
      "Batch Loss:45.54777908325195 at iter: 188000\n",
      "Batch Loss:55.93721008300781 at iter: 189000\n",
      "Running Avg Loss:2.314309386151284 at epoch: 0\n",
      " Kayshan Kelyandh Gaug Torre Zanelh Kadam Jorijun Daydin Kileen Kodlur Majen Korsiqa Threh Kariann Kaydel Iccei Kaidi Darek Krion Kashek Keaba Avhon Levest Jhaynelluon Kanit Yalil Haalcrgan Coray Eera\n",
      "Batch Loss:61.33598327636719 at iter: 190000\n",
      "Batch Loss:65.59626007080078 at iter: 191000\n",
      "Batch Loss:53.663814544677734 at iter: 192000\n",
      "Batch Loss:59.866817474365234 at iter: 193000\n",
      "Batch Loss:59.147518157958984 at iter: 194000\n",
      "Batch Loss:61.12261962890625 at iter: 195000\n",
      "Batch Loss:64.26261901855469 at iter: 196000\n",
      "Batch Loss:63.40211486816406 at iter: 197000\n",
      "Batch Loss:65.84588623046875 at iter: 198000\n",
      "Batch Loss:56.558433532714844 at iter: 199000\n",
      "Running Avg Loss:2.354723993209377 at epoch: 0\n",
      "n Angude Jokca Zarahlax Vina Amaro Dimaely Abbale Thek Kymat Addret cayrey Kely JaufHanelini Raymen TonmyTey Ryxnn Oyadeliany Zoydin Ceton Tovani Alyo Tamaew Bayien Vynulec Parusing Zkerayz Vaneliel N\n",
      "Batch Loss:57.641082763671875 at iter: 200000\n",
      "Batch Loss:49.263179779052734 at iter: 201000\n",
      "Batch Loss:65.45122528076172 at iter: 202000\n",
      "Batch Loss:60.27656173706055 at iter: 203000\n",
      "Batch Loss:70.13580322265625 at iter: 204000\n",
      "Batch Loss:66.90513610839844 at iter: 205000\n",
      "Batch Loss:48.7449836730957 at iter: 206000\n",
      "Batch Loss:62.89349365234375 at iter: 207000\n",
      "Batch Loss:51.958003997802734 at iter: 208000\n",
      "Batch Loss:42.89318084716797 at iter: 209000\n",
      "Running Avg Loss:2.240167369170487 at epoch: 0\n",
      "o Lelon Kowt Sirison Kandr Kennis Damler Kilee Ilveteen Kanden Khadler Nyshay Irthmin Kaiton Kaydhie Mahylen Keleon Lipven Kanzen Jahker Keben Krubelas Gohtten Kahmitn Grayz Damanthenn Mard Javmwee Jm\n",
      "Batch Loss:45.46772384643555 at iter: 210000\n",
      "Batch Loss:60.79411697387695 at iter: 211000\n",
      "Batch Loss:54.0279426574707 at iter: 212000\n",
      "Batch Loss:48.960411071777344 at iter: 213000\n",
      "Batch Loss:43.96009063720703 at iter: 214000\n",
      "Batch Loss:58.44113540649414 at iter: 215000\n",
      "Batch Loss:57.247901916503906 at iter: 216000\n",
      "Batch Loss:55.74449920654297 at iter: 217000\n",
      "Batch Loss:55.1221923828125 at iter: 218000\n",
      "Batch Loss:58.28583908081055 at iter: 219000\n",
      "Running Avg Loss:2.249494272097759 at epoch: 0\n",
      " Suleny MykaKerinar Lerzmen Maqpan zrel  Mantti Mat Ragk Leohel Leyan Maymery FreynLez Kavion  yir Mirxen Z Hann rrilgLe Prktel Bislam Kimon Kyphelr Erray Lannith Lonntaan Jolchur Seono Layy Mabdvin L\n",
      "Batch Loss:58.72724533081055 at iter: 220000\n",
      "Batch Loss:60.67039108276367 at iter: 221000\n",
      "Batch Loss:61.244773864746094 at iter: 222000\n",
      "Batch Loss:56.30341339111328 at iter: 223000\n",
      "Batch Loss:57.580196380615234 at iter: 224000\n",
      "Batch Loss:55.262203216552734 at iter: 225000\n",
      "Batch Loss:49.984474182128906 at iter: 226000\n",
      "Batch Loss:52.933921813964844 at iter: 227000\n",
      "Batch Loss:62.762325286865234 at iter: 228000\n",
      "Batch Loss:40.50773620605469 at iter: 229000\n",
      "Running Avg Loss:2.228554259589687 at epoch: 0\n",
      "a Jaiseon Fhallmahltri Jharielo Mande Thangte Jusshuw Kandenn Janile Jahen Jwepn Tero Konyer Jamon Karyin Judairo Baston Jasane JeyouGaiem Hovin Jokideb Ducatie Damiar Juiodan Jafuen Juvohon Hyskeyno \n",
      "Batch Loss:53.06093215942383 at iter: 230000\n",
      "Batch Loss:55.88383483886719 at iter: 231000\n",
      "Batch Loss:60.447120666503906 at iter: 232000\n",
      "Batch Loss:80.4069595336914 at iter: 233000\n",
      "Batch Loss:67.79961395263672 at iter: 234000\n",
      "Batch Loss:64.26484680175781 at iter: 235000\n",
      "Batch Loss:55.81620788574219 at iter: 236000\n",
      "Batch Loss:47.0926628112793 at iter: 237000\n",
      "Batch Loss:60.80546569824219 at iter: 1000\n",
      "Batch Loss:51.314361572265625 at iter: 2000\n",
      "Batch Loss:61.74893569946289 at iter: 3000\n",
      "Batch Loss:49.50754928588867 at iter: 4000\n",
      "Batch Loss:54.49522399902344 at iter: 5000\n",
      "Batch Loss:53.168418884277344 at iter: 6000\n",
      "Batch Loss:50.24582290649414 at iter: 7000\n",
      "Batch Loss:47.94879913330078 at iter: 8000\n",
      "Batch Loss:57.17469024658203 at iter: 9000\n",
      "Running Avg Loss:2.1168486474160106 at epoch: 1\n",
      "inlatoh ninzicKa Mirs Lynveim Laylew Joxina Kyncarcha Linand Lysnne Kees Ana Sastych Karin KetriniaNa Liaynn KesanGee Eulatt Manzie Wriellia Gadgyen Kiadri Ryn Sar Riona Marza It Seelle Lalle Vadrin S\n",
      "Batch Loss:54.778114318847656 at iter: 10000\n",
      "Batch Loss:57.214847564697266 at iter: 11000\n",
      "Batch Loss:57.944847106933594 at iter: 12000\n",
      "Batch Loss:57.08481979370117 at iter: 13000\n",
      "Batch Loss:56.7921142578125 at iter: 14000\n",
      "Batch Loss:51.65238952636719 at iter: 15000\n",
      "Batch Loss:65.25347137451172 at iter: 16000\n",
      "Batch Loss:49.591766357421875 at iter: 17000\n",
      "Batch Loss:55.9632568359375 at iter: 18000\n",
      "Batch Loss:46.12669372558594 at iter: 19000\n",
      "Running Avg Loss:2.0714746162129565 at epoch: 1\n",
      " Iveci Maro Kairaynte Gisab Jotody Reya Aiala Aenlinta Ayah Saryn Tinsema Noverah Arahlyn Mayla Nemia Emalyra Anflre Khne Tinth Mariton Sandynn Kenniiha Variah Trian Gilynna Camari Jamine Byolie Jodan\n",
      "Batch Loss:58.51552200317383 at iter: 20000\n",
      "Batch Loss:53.13734817504883 at iter: 21000\n",
      "Batch Loss:54.31575393676758 at iter: 22000\n",
      "Batch Loss:50.12247848510742 at iter: 23000\n",
      "Batch Loss:60.27218246459961 at iter: 24000\n",
      "Batch Loss:55.662506103515625 at iter: 25000\n",
      "Batch Loss:49.53325271606445 at iter: 26000\n",
      "Batch Loss:45.202667236328125 at iter: 27000\n",
      "Batch Loss:54.5198860168457 at iter: 28000\n",
      "Batch Loss:46.36159133911133 at iter: 29000\n",
      "Running Avg Loss:2.0498720895946025 at epoch: 1\n",
      " Kallanulya Milbel Jeige Lylyn Oliorit Evelynn Kalle Macrane Aillone Dalynna Gethrii Del Lynoe Momyah Raeth Serilra Arissee Lanay Mallyn Nayleel Kirralia Masqura Hnerym Myna Briessie Javiynn Maleia M \n",
      "Batch Loss:45.13721466064453 at iter: 30000\n",
      "Batch Loss:53.29304885864258 at iter: 31000\n",
      "Batch Loss:46.622802734375 at iter: 32000\n",
      "Batch Loss:46.72127151489258 at iter: 33000\n",
      "Batch Loss:53.0328254699707 at iter: 34000\n",
      "Batch Loss:57.01172637939453 at iter: 35000\n",
      "Batch Loss:47.28512954711914 at iter: 36000\n",
      "Batch Loss:59.03500747680664 at iter: 37000\n",
      "Batch Loss:52.26327896118164 at iter: 38000\n",
      "Batch Loss:47.026832580566406 at iter: 39000\n",
      "Running Avg Loss:2.0938096143059433 at epoch: 1\n",
      " Makayla Namaha Jarncile Malilri Thave Averna Alah Jayna Areelannae Andyrowa Caritem Aandia Zanaca Mckyton Niyna Rossynn Leige Reqahnia Mara IzaylLe Mielly Rima Yeyna Lija Alie Juja Grenarce Callina A\n",
      "Batch Loss:54.653594970703125 at iter: 40000\n",
      "Batch Loss:62.832489013671875 at iter: 41000\n",
      "Batch Loss:61.0389518737793 at iter: 42000\n",
      "Batch Loss:61.90591049194336 at iter: 43000\n",
      "Batch Loss:44.48713302612305 at iter: 44000\n",
      "Batch Loss:59.73875427246094 at iter: 45000\n",
      "Batch Loss:52.743011474609375 at iter: 46000\n",
      "Batch Loss:50.17887496948242 at iter: 47000\n",
      "Batch Loss:55.8108024597168 at iter: 48000\n",
      "Batch Loss:47.84254837036133 at iter: 49000\n",
      "Running Avg Loss:2.1041177696909754 at epoch: 1\n",
      " Jaelosey Sisanah Tylee Keisine Yevayah Benny Elleelinnee Ramya Jestta Elydny Gis Wenia Sammarha Jeanayca Cnoreinn Mothnyy Zamori Caly Rozily Nileliah Malezia Srrinalie Sarylea Lenne Catlee Addea Zaeo\n",
      "Batch Loss:61.05412292480469 at iter: 50000\n",
      "Batch Loss:53.88323974609375 at iter: 51000\n",
      "Batch Loss:37.62660598754883 at iter: 52000\n",
      "Batch Loss:44.297752380371094 at iter: 53000\n",
      "Batch Loss:55.403682708740234 at iter: 54000\n",
      "Batch Loss:53.889041900634766 at iter: 55000\n",
      "Batch Loss:43.57450866699219 at iter: 56000\n",
      "Batch Loss:56.959014892578125 at iter: 57000\n",
      "Batch Loss:55.414337158203125 at iter: 58000\n",
      "Batch Loss:51.501304626464844 at iter: 59000\n",
      "Running Avg Loss:2.09720473895818 at epoch: 1\n",
      "a Jeza Elllina Kerycpeeni Jlie Nivano Zishlano Hanvia Mida Jah Aicer Selxia Aijalian Rarya Serlyn Varilinn Zherin Midahka Nisslew Shelee Rolia Landalah Crrinn Mero LaZuma Rary Surra Matla Nen LoZorni \n",
      "Batch Loss:49.93113708496094 at iter: 60000\n",
      "Batch Loss:44.37375259399414 at iter: 61000\n",
      "Batch Loss:51.648658752441406 at iter: 62000\n",
      "Batch Loss:52.00926971435547 at iter: 63000\n",
      "Batch Loss:53.19315719604492 at iter: 64000\n",
      "Batch Loss:46.190425872802734 at iter: 65000\n",
      "Batch Loss:45.21685791015625 at iter: 66000\n",
      "Batch Loss:46.368709564208984 at iter: 67000\n",
      "Batch Loss:51.66263198852539 at iter: 68000\n",
      "Batch Loss:44.94416427612305 at iter: 69000\n",
      "Running Avg Loss:2.094408415444009 at epoch: 1\n",
      "eel Yarilynn Akina Emmiri Afriellynara Brieen Beinnuell Yonelel Fasadiahle Yailyn Maellah Calyn Paita Dimayshy Dunah Gemiria Delody Emari Zoeimie Elaubila Zonaya Hanila Avanna Damalian Galdel Imariy D\n",
      "Batch Loss:41.560272216796875 at iter: 70000\n",
      "Batch Loss:49.35783004760742 at iter: 71000\n",
      "Batch Loss:57.784934997558594 at iter: 72000\n",
      "Batch Loss:41.41307830810547 at iter: 73000\n",
      "Batch Loss:47.48539352416992 at iter: 74000\n",
      "Batch Loss:49.40492630004883 at iter: 75000\n",
      "Batch Loss:60.74962615966797 at iter: 76000\n",
      "Batch Loss:63.39329528808594 at iter: 77000\n",
      "Batch Loss:48.37250900268555 at iter: 78000\n",
      "Batch Loss:38.07102966308594 at iter: 79000\n",
      "Running Avg Loss:2.085056761750765 at epoch: 1\n",
      "ia Yanyr Bayah Berani Liya Anaylee Toeve Azmannah Andie Azatey Deralynna Raerie Breia Adeia Da Ileiza Tamia Anzizea Bikalia Sassria Aghi Analy Ayleeza Amoria Adyasy Bkylio Becaiillea Tambre Akisee Cor\n",
      "Batch Loss:49.223331451416016 at iter: 80000\n",
      "Batch Loss:50.80442810058594 at iter: 81000\n",
      "Batch Loss:56.57067108154297 at iter: 82000\n",
      "Batch Loss:49.99350357055664 at iter: 83000\n",
      "Batch Loss:59.628360748291016 at iter: 84000\n",
      "Batch Loss:46.36832046508789 at iter: 85000\n",
      "Batch Loss:44.352779388427734 at iter: 86000\n",
      "Batch Loss:72.72276306152344 at iter: 87000\n",
      "Batch Loss:48.68232345581055 at iter: 88000\n",
      "Batch Loss:43.363712310791016 at iter: 89000\n",
      "Running Avg Loss:2.058606553607341 at epoch: 1\n",
      "Malyrah TalisItilyn Kaisa Goupiy Liefly Jawun Jahein Jaza Losleya Joal Daalex Emirlenza Malyah Keleen Mixsone Marvita Jelelina Janix Caileshiess Jlanial Jayoch Jopale Jorm Gyritone Jorovente Letaba Je\n",
      "Batch Loss:45.68900680541992 at iter: 90000\n",
      "Batch Loss:41.9366569519043 at iter: 91000\n",
      "Batch Loss:57.28099822998047 at iter: 92000\n",
      "Batch Loss:52.839698791503906 at iter: 93000\n",
      "Batch Loss:57.8785285949707 at iter: 94000\n",
      "Batch Loss:39.4888916015625 at iter: 95000\n",
      "Batch Loss:49.72317886352539 at iter: 96000\n",
      "Batch Loss:44.9732666015625 at iter: 97000\n",
      "Batch Loss:53.13439178466797 at iter: 98000\n",
      "Batch Loss:49.471412658691406 at iter: 99000\n",
      "Running Avg Loss:2.0432430983794854 at epoch: 1\n",
      " Khyley Jaynnia Kamveta Keliah Kayleis K KenaHa Kmin Katire Kirpelene Khia Josalyne Kat Mized Kendra Kazli Harlia Jafe Khnnen Khmilene Lasienn Khie Kenebec Szailya KelinnoKe Kyay Karpelley Khany Bleel\n",
      "Batch Loss:48.43144989013672 at iter: 100000\n",
      "Batch Loss:48.30392837524414 at iter: 101000\n",
      "Batch Loss:50.907291412353516 at iter: 102000\n",
      "Batch Loss:48.581886291503906 at iter: 103000\n",
      "Batch Loss:56.20747375488281 at iter: 104000\n",
      "Batch Loss:73.6873550415039 at iter: 105000\n",
      "Batch Loss:37.822078704833984 at iter: 106000\n",
      "Batch Loss:45.23958206176758 at iter: 107000\n",
      "Batch Loss:49.133121490478516 at iter: 108000\n",
      "Batch Loss:43.35425567626953 at iter: 109000\n",
      "Running Avg Loss:2.049288148025796 at epoch: 1\n",
      "Emmaui Mahriya Estaima Ekramani Danni Evelh Erleyla Delynn Emmiarea Preo Emarnte Pryllenn Livaria Elinna Emmiriah Melline Brypan Dessane Emawliah Elya EnleyMaaysep Jllvila Ellya Emalie Lyrina Embibe H\n",
      "Batch Loss:54.603599548339844 at iter: 110000\n",
      "Batch Loss:55.237789154052734 at iter: 111000\n",
      "Batch Loss:47.13041687011719 at iter: 112000\n",
      "Batch Loss:40.14013671875 at iter: 113000\n",
      "Batch Loss:37.41633224487305 at iter: 114000\n",
      "Batch Loss:48.70423126220703 at iter: 115000\n",
      "Batch Loss:61.85554504394531 at iter: 116000\n",
      "Batch Loss:56.27617263793945 at iter: 117000\n",
      "Batch Loss:48.559322357177734 at iter: 118000\n",
      "Batch Loss:49.33918762207031 at iter: 119000\n",
      "Running Avg Loss:2.0747488386491315 at epoch: 1\n",
      "a Zalosina Abightte AnnethlaZdashabbbrTadilin Dujosante Azmyo Kyamie Malayahe Vija Adilyan AnntijZeobelle Fay Lyoza Alerynsiy Zicia Aviyah Avikia Asmi Celraytahdree AdrinziaZoshana Adria Aadudienna Az\n",
      "Batch Loss:51.3530158996582 at iter: 120000\n",
      "Batch Loss:47.44696044921875 at iter: 121000\n",
      "Batch Loss:51.43516159057617 at iter: 122000\n",
      "Batch Loss:45.53586196899414 at iter: 123000\n",
      "Batch Loss:45.45404052734375 at iter: 124000\n",
      "Batch Loss:54.315189361572266 at iter: 125000\n",
      "Batch Loss:62.318180084228516 at iter: 126000\n",
      "Batch Loss:61.39462661743164 at iter: 127000\n",
      "Batch Loss:42.089447021484375 at iter: 128000\n",
      "Batch Loss:48.30612564086914 at iter: 129000\n",
      "Running Avg Loss:1.981128788209427 at epoch: 1\n",
      "en Keenle Kezlya Kehrlit Kakin Jushamenis Kander Kennavohane Korra Jaynde Kahnnelen Kammariston Kena Kiahlli Keazaia Jizabbesyn Kushidelynu Kammin Keebina Keldra KeannJoag Kayzen Hahlyn Kemarie Kanary\n",
      "Batch Loss:50.18975067138672 at iter: 130000\n",
      "Batch Loss:36.812255859375 at iter: 131000\n",
      "Batch Loss:48.43000030517578 at iter: 132000\n",
      "Batch Loss:44.628047943115234 at iter: 133000\n",
      "Batch Loss:46.11552429199219 at iter: 134000\n",
      "Batch Loss:52.84608840942383 at iter: 135000\n",
      "Batch Loss:59.46752166748047 at iter: 136000\n",
      "Batch Loss:71.587890625 at iter: 137000\n",
      "Batch Loss:58.762451171875 at iter: 138000\n",
      "Batch Loss:48.65886688232422 at iter: 139000\n",
      "Running Avg Loss:2.070343168424442 at epoch: 1\n",
      "yn Lyline Telerikh Sanergh Zomiera Nakaryah Masori Thigh Khaniriah Dumselon Zukayna Zatarah Vaythlyah Kemien Kerahmie Fayla Damani Anie Gwelldya Zanivanligra Zoparyn Zabruss Neena Wityah Isea Cavito K\n",
      "Batch Loss:57.95635986328125 at iter: 140000\n",
      "Batch Loss:52.55603790283203 at iter: 141000\n",
      "Batch Loss:60.07558059692383 at iter: 142000\n",
      "Batch Loss:51.2247200012207 at iter: 143000\n",
      "Batch Loss:59.84492874145508 at iter: 144000\n",
      "Batch Loss:59.1727180480957 at iter: 145000\n",
      "Batch Loss:56.631591796875 at iter: 146000\n",
      "Batch Loss:49.45663833618164 at iter: 147000\n",
      "Batch Loss:61.885013580322266 at iter: 148000\n",
      "Batch Loss:60.29807662963867 at iter: 149000\n",
      "Running Avg Loss:2.2365706427415835 at epoch: 1\n",
      " Gamardyr Brobe Maidun Edden Geon Havon Ayman Cora Zaimy Isham Jyiae Erman Hayrynah Khire Zadaerrynn Yten Pael Rickoe Jaem Fanarn Breet Kelon Jeh Gashan Shulli Novor Dyran Kacyluc Bancir Kambryd Deman\n",
      "Batch Loss:55.589378356933594 at iter: 150000\n",
      "Batch Loss:55.67558288574219 at iter: 151000\n",
      "Batch Loss:52.83454132080078 at iter: 152000\n",
      "Batch Loss:55.905357360839844 at iter: 153000\n",
      "Batch Loss:57.83454513549805 at iter: 154000\n",
      "Batch Loss:69.68225860595703 at iter: 155000\n",
      "Batch Loss:59.89828109741211 at iter: 156000\n",
      "Batch Loss:61.14937210083008 at iter: 157000\n",
      "Batch Loss:62.27806091308594 at iter: 158000\n",
      "Batch Loss:54.02066421508789 at iter: 159000\n",
      "Running Avg Loss:2.2038030406715348 at epoch: 1\n",
      "check Haasitol Naon Ost Jassin Netat Fahut Lyyn Jeeldyn Raheel Machel Roas Ouchark Ramenk Nadon Sting Sadel Nleys Maceles Rukehan Medrek Joven Jakilon Olen Lilton Juksov Welpon Ebevin Aantsr DaraemaJa\n",
      "Batch Loss:49.75491714477539 at iter: 160000\n",
      "Batch Loss:51.019020080566406 at iter: 161000\n",
      "Batch Loss:52.14314270019531 at iter: 162000\n",
      "Batch Loss:60.42039108276367 at iter: 163000\n",
      "Batch Loss:60.0112419128418 at iter: 164000\n",
      "Batch Loss:48.878116607666016 at iter: 165000\n",
      "Batch Loss:60.54772186279297 at iter: 166000\n",
      "Batch Loss:48.51511001586914 at iter: 167000\n",
      "Batch Loss:52.239627838134766 at iter: 168000\n",
      "Batch Loss:69.9909896850586 at iter: 169000\n",
      "Running Avg Loss:2.208004130211845 at epoch: 1\n",
      " Saria Levin Chamani Kayven Davius Andin Carico Crasson Kadaion Jaiffon Ickon Ayshet Jadtim Levon Braidt Faynia KelloUd Eziyla Coled Emasan Judiket Kammerudy W Prev Rashie Ri Najem Misstis Salishay Za\n",
      "Batch Loss:56.686126708984375 at iter: 170000\n",
      "Batch Loss:56.30207443237305 at iter: 171000\n",
      "Batch Loss:52.03538131713867 at iter: 172000\n",
      "Batch Loss:58.44585418701172 at iter: 173000\n",
      "Batch Loss:49.19533157348633 at iter: 174000\n",
      "Batch Loss:59.623252868652344 at iter: 175000\n",
      "Batch Loss:46.04651641845703 at iter: 176000\n",
      "Batch Loss:53.635799407958984 at iter: 177000\n",
      "Batch Loss:59.535606384277344 at iter: 178000\n",
      "Batch Loss:54.54670715332031 at iter: 179000\n",
      "Running Avg Loss:2.224645381631516 at epoch: 1\n",
      "r Nocawal Rydan Tream Wiel Growat Javion Jayser Thav Skayn Inum Ensh Khxon Kaan Jeramen Koyamolan Izcalvy Shioliam Juston Kendio Ensean Jexonir On Fleirsyko Muoalal wagolod Etian Fouter Ausan Khen Dov\n",
      "Batch Loss:54.0833740234375 at iter: 180000\n",
      "Batch Loss:62.1173210144043 at iter: 181000\n",
      "Batch Loss:68.62395477294922 at iter: 182000\n",
      "Batch Loss:56.64628219604492 at iter: 183000\n",
      "Batch Loss:46.55878829956055 at iter: 184000\n",
      "Batch Loss:52.65531539916992 at iter: 185000\n",
      "Batch Loss:60.68207550048828 at iter: 186000\n",
      "Batch Loss:58.63603973388672 at iter: 187000\n",
      "Batch Loss:46.71553039550781 at iter: 188000\n",
      "Batch Loss:51.85651779174805 at iter: 189000\n",
      "Running Avg Loss:2.2525150791535156 at epoch: 1\n",
      " Zerisc Kaybhian CaralmoKiaee KobbeKyton Jamafy Kebim Kengan Kaida Kaxrey Jacil Grallan Cordan Joh Abrs Andelin Girrven Kusser Keyond Kondyr Keyestan JarjuanQKin Kayne Kemgr Khawn Geritcok Kanzlel Jas\n",
      "Batch Loss:65.56314086914062 at iter: 190000\n",
      "Batch Loss:63.18330383300781 at iter: 191000\n",
      "Batch Loss:51.855350494384766 at iter: 192000\n",
      "Batch Loss:56.29594421386719 at iter: 193000\n",
      "Batch Loss:58.39663314819336 at iter: 194000\n",
      "Batch Loss:58.260555267333984 at iter: 195000\n",
      "Batch Loss:63.628082275390625 at iter: 196000\n",
      "Batch Loss:60.52366638183594 at iter: 197000\n",
      "Batch Loss:63.98781967163086 at iter: 198000\n",
      "Batch Loss:50.44208908081055 at iter: 199000\n",
      "Running Avg Loss:2.2775101975791157 at epoch: 1\n",
      "te Zaikik Chika Beleyn Millen Yuteoen Ardlich Zeyter Zaf Rachanna Zariah Z Nirri Xokanie Marik Pheiian Zaikan Breyvocauly Zokeoh Zavaysten Bramax Currin Trhonu Zaathan Chyfen Shjo Vebiat Zaiton Yucken\n",
      "Batch Loss:56.78407287597656 at iter: 200000\n",
      "Batch Loss:45.361717224121094 at iter: 201000\n",
      "Batch Loss:62.1451301574707 at iter: 202000\n",
      "Batch Loss:54.6744384765625 at iter: 203000\n",
      "Batch Loss:63.44575119018555 at iter: 204000\n",
      "Batch Loss:65.5786361694336 at iter: 205000\n",
      "Batch Loss:47.87025833129883 at iter: 206000\n",
      "Batch Loss:60.44464111328125 at iter: 207000\n",
      "Batch Loss:50.16911315917969 at iter: 208000\n",
      "Batch Loss:40.353431701660156 at iter: 209000\n",
      "Running Avg Loss:2.1674529770443214 at epoch: 1\n",
      "c Karrish Kahenx Kes Emar Raylan Jachitil Finnavirlan Fonthreto Jaajtin Kraan Kaci Huanle Jeck Jylerak Keyn Kaefraben Kaydir Kaioh Kaesan Kyce Jesan Kydah Remolyn Keelon Maeil Kaglas Kenten Kegon Kayl\n",
      "Batch Loss:45.877750396728516 at iter: 210000\n",
      "Batch Loss:55.957794189453125 at iter: 211000\n",
      "Batch Loss:53.44419860839844 at iter: 212000\n",
      "Batch Loss:46.46747970581055 at iter: 213000\n",
      "Batch Loss:40.98283004760742 at iter: 214000\n",
      "Batch Loss:58.23002243041992 at iter: 215000\n",
      "Batch Loss:56.46057891845703 at iter: 216000\n",
      "Batch Loss:53.30807113647461 at iter: 217000\n",
      "Batch Loss:54.37482452392578 at iter: 218000\n",
      "Batch Loss:54.35987854003906 at iter: 219000\n",
      "Running Avg Loss:2.183659699951112 at epoch: 1\n",
      "m Salee Macdof Sadon Loemer Ngori Otynder Kellin Linastiel Makman Pyll Lellees Rangat Mahton Mockell P Geare Luuen Mock Gabran Laydyn Melisy Matwunk Astakmeshy Hyniss Maciev Tyan Mackan Thutpi Lyde Ma\n",
      "Batch Loss:55.47300720214844 at iter: 220000\n",
      "Batch Loss:61.625877380371094 at iter: 221000\n",
      "Batch Loss:57.90592956542969 at iter: 222000\n",
      "Batch Loss:57.05976867675781 at iter: 223000\n",
      "Batch Loss:58.07664108276367 at iter: 224000\n",
      "Batch Loss:53.51251220703125 at iter: 225000\n",
      "Batch Loss:47.029945373535156 at iter: 226000\n",
      "Batch Loss:52.15959548950195 at iter: 227000\n",
      "Batch Loss:59.746498107910156 at iter: 228000\n",
      "Batch Loss:38.41410446166992 at iter: 229000\n",
      "Running Avg Loss:2.1747923857457936 at epoch: 1\n",
      "an Jamalic Justey Jalu Ke Buvyna Jubius Jhkan Jausev Lewin Jakkenph Izar Joji JamerIssa Lahman Josanias LinaelJel Is Crataan Jerraaru Jeemelie Jozieas Jolivid Jondaide Jarcis Jahadi Keleduenoh Carien \n",
      "Batch Loss:53.301761627197266 at iter: 230000\n",
      "Batch Loss:51.90488052368164 at iter: 231000\n",
      "Batch Loss:60.556087493896484 at iter: 232000\n",
      "Batch Loss:81.55799865722656 at iter: 233000\n",
      "Batch Loss:69.23356628417969 at iter: 234000\n",
      "Batch Loss:64.49105834960938 at iter: 235000\n",
      "Batch Loss:56.38386535644531 at iter: 236000\n",
      "Batch Loss:43.4975700378418 at iter: 237000\n",
      "Batch Loss:60.83611297607422 at iter: 1000\n",
      "Batch Loss:52.020042419433594 at iter: 2000\n",
      "Batch Loss:56.58379364013672 at iter: 3000\n",
      "Batch Loss:50.069602966308594 at iter: 4000\n",
      "Batch Loss:54.217002868652344 at iter: 5000\n",
      "Batch Loss:53.83162307739258 at iter: 6000\n",
      "Batch Loss:48.36711120605469 at iter: 7000\n",
      "Batch Loss:48.73521423339844 at iter: 8000\n",
      "Batch Loss:56.82277297973633 at iter: 9000\n",
      "Running Avg Loss:2.0786724015399813 at epoch: 2\n",
      "ilae Gieenna Aysey Darly Malltyle Amari Biellera SaPrebasce Ludahe Mors Heem Samyna Apgrahna Lowandei Kaishava Ariee Avena Aina Cvelen Kaima Chah Caled Quonns Abakee Glyncia Varaylsh Huryde Jabera Kel\n",
      "Batch Loss:52.81684112548828 at iter: 10000\n",
      "Batch Loss:57.19276809692383 at iter: 11000\n",
      "Batch Loss:54.03369903564453 at iter: 12000\n",
      "Batch Loss:54.571598052978516 at iter: 13000\n",
      "Batch Loss:53.26865768432617 at iter: 14000\n",
      "Batch Loss:50.25509262084961 at iter: 15000\n",
      "Batch Loss:65.63897705078125 at iter: 16000\n",
      "Batch Loss:44.60297775268555 at iter: 17000\n",
      "Batch Loss:52.157196044921875 at iter: 18000\n",
      "Batch Loss:45.15565872192383 at iter: 19000\n",
      "Running Avg Loss:2.0096161217942834 at epoch: 2\n",
      "aioh Jezlyn Faionna Aederiso Delyah Ampalea Amara Alyce Laylyn Kilyn Jayan Jeyrauith Aaniyah Anadou Neelyn Yani Jadya Iagryon Mavine baahar Saynerl Murley Suva Neslyn Miea Leven Howra Shmurfe Sivian V\n",
      "Batch Loss:57.645076751708984 at iter: 20000\n",
      "Batch Loss:51.06336975097656 at iter: 21000\n",
      "Batch Loss:51.22057342529297 at iter: 22000\n",
      "Batch Loss:49.62394714355469 at iter: 23000\n",
      "Batch Loss:57.9102668762207 at iter: 24000\n",
      "Batch Loss:51.236446380615234 at iter: 25000\n",
      "Batch Loss:45.8500862121582 at iter: 26000\n",
      "Batch Loss:40.93199920654297 at iter: 27000\n",
      "Batch Loss:55.15621566772461 at iter: 28000\n",
      "Batch Loss:43.744590759277344 at iter: 29000\n",
      "Running Avg Loss:1.9646726728485897 at epoch: 2\n",
      "a Narish Mellyy Nyto Shabe Zayssal Zylee Mahaure Rous Marych Luziya Lislynn Ling Briabra Lawinde Jallyah Mikanna Jacebla Mayiana Orialaa Ruaianna MakcelyndKhishah Khiena Majanly Mecelleigh Laki Caix S\n",
      "Batch Loss:38.3047981262207 at iter: 30000\n",
      "Batch Loss:49.72652053833008 at iter: 31000\n",
      "Batch Loss:42.88258743286133 at iter: 32000\n",
      "Batch Loss:44.66753005981445 at iter: 33000\n",
      "Batch Loss:48.95286560058594 at iter: 34000\n",
      "Batch Loss:58.82294845581055 at iter: 35000\n",
      "Batch Loss:47.00638198852539 at iter: 36000\n",
      "Batch Loss:61.05588150024414 at iter: 37000\n",
      "Batch Loss:48.760963439941406 at iter: 38000\n",
      "Batch Loss:46.06466293334961 at iter: 39000\n",
      "Running Avg Loss:2.0199818058541044 at epoch: 2\n",
      "ciah Aleyyah AaliQue Zatimalee Zaea Yesel Jasilja Karikaira Mevon Likyna Suigha Snaia Rickanlennh Quanna Koristyna Sabirncka Olemiza Nophva Thana Selina Aarynt Anayra Andreizabiaza Masta Lakella Marde\n",
      "Batch Loss:53.94490432739258 at iter: 40000\n",
      "Batch Loss:59.90168762207031 at iter: 41000\n",
      "Batch Loss:59.006317138671875 at iter: 42000\n",
      "Batch Loss:60.85565948486328 at iter: 43000\n",
      "Batch Loss:43.347084045410156 at iter: 44000\n",
      "Batch Loss:53.92972183227539 at iter: 45000\n",
      "Batch Loss:51.05389404296875 at iter: 46000\n",
      "Batch Loss:49.38815689086914 at iter: 47000\n",
      "Batch Loss:53.33673858642578 at iter: 48000\n",
      "Batch Loss:44.43633270263672 at iter: 49000\n",
      "Running Avg Loss:2.0196708597125483 at epoch: 2\n",
      "ey Capry Annilia Antseise Jelaha Kirimiya Onicenelia Antely Avayna Arannia Annalinileth Anlee Sammie Stophami Ve Lazarlee Aessi Daynnne ArilyKa Anaeleliana Elubie Ahdelik Anary Daseria Aahri Alyanna C\n",
      "Batch Loss:60.251590728759766 at iter: 50000\n",
      "Batch Loss:50.87961959838867 at iter: 51000\n",
      "Batch Loss:40.902339935302734 at iter: 52000\n",
      "Batch Loss:45.75797653198242 at iter: 53000\n",
      "Batch Loss:54.54195785522461 at iter: 54000\n",
      "Batch Loss:47.215087890625 at iter: 55000\n",
      "Batch Loss:43.6689338684082 at iter: 56000\n",
      "Batch Loss:55.66196060180664 at iter: 57000\n",
      "Batch Loss:55.76603698730469 at iter: 58000\n",
      "Batch Loss:47.85042190551758 at iter: 59000\n",
      "Running Avg Loss:2.0538939128370957 at epoch: 2\n",
      " Donne Eivi Lorie Mouah Mathslee Prinie Lullila Teely Arihena Masy Maxkarie Mersae Naidakh Melmia Nebhelin Nahtona Nijah J ieali Kattha Naquber Braly Kyday Moveer Brabenz Lolii Lyndy Laniyl Shanye Sha\n",
      "Batch Loss:48.72944259643555 at iter: 60000\n",
      "Batch Loss:44.00297927856445 at iter: 61000\n",
      "Batch Loss:49.46630096435547 at iter: 62000\n",
      "Batch Loss:47.07838439941406 at iter: 63000\n",
      "Batch Loss:52.77320098876953 at iter: 64000\n",
      "Batch Loss:43.00436019897461 at iter: 65000\n",
      "Batch Loss:41.142642974853516 at iter: 66000\n",
      "Batch Loss:50.66325759887695 at iter: 67000\n",
      "Batch Loss:48.42060470581055 at iter: 68000\n",
      "Batch Loss:43.04736328125 at iter: 69000\n",
      "Running Avg Loss:2.04436248368714 at epoch: 2\n",
      "ari Abilawe JorraAviah Cavi Coma Cheyn Evhe Ev HanelFa Ariah Bricel Cale Duralyat Nombell Corla Dariema Khteen Taseel Lical Joristha Jaynirany Paetinn iona Briynze Kaelen Yackey Charyn Evannaley Y Mer\n",
      "Batch Loss:43.304080963134766 at iter: 70000\n",
      "Batch Loss:49.20624923706055 at iter: 71000\n",
      "Batch Loss:57.70609664916992 at iter: 72000\n",
      "Batch Loss:40.34526443481445 at iter: 73000\n",
      "Batch Loss:44.67038345336914 at iter: 74000\n",
      "Batch Loss:48.24072265625 at iter: 75000\n",
      "Batch Loss:57.95610809326172 at iter: 76000\n",
      "Batch Loss:60.54881286621094 at iter: 77000\n",
      "Batch Loss:46.36705017089844 at iter: 78000\n",
      "Batch Loss:39.492759704589844 at iter: 79000\n",
      "Running Avg Loss:2.0427709301715717 at epoch: 2\n",
      "e Damari Kylha Aahlynne Djeigh Avoeduwi Arrianna Dieleu ChanaigDana Aoree Azbiya Aimmysi Careyaha Aviri Anzia Balia Braeky Duiat Arannit Berah Bbee Besaja Breea Beylaska Aabelle Baxania Emeyl Carieyn \n",
      "Batch Loss:49.23897171020508 at iter: 80000\n",
      "Batch Loss:49.46373748779297 at iter: 81000\n",
      "Batch Loss:53.659934997558594 at iter: 82000\n",
      "Batch Loss:48.29861068725586 at iter: 83000\n",
      "Batch Loss:57.75424575805664 at iter: 84000\n",
      "Batch Loss:47.313236236572266 at iter: 85000\n",
      "Batch Loss:45.53759002685547 at iter: 86000\n",
      "Batch Loss:68.5795669555664 at iter: 87000\n",
      "Batch Loss:46.62931442260742 at iter: 88000\n",
      "Batch Loss:43.88444519042969 at iter: 89000\n",
      "Running Avg Loss:2.0169154110260306 at epoch: 2\n",
      "Qurah Koyna Javyni Kiona Jupeagh Kamiath Dimah Jumina JoranaLa Jammarize Kayli Suraiget Jahaysa KarsiKeya Karime Jraviela Jazlyan Jahyiah Kamish Kenzee Kama Haelana Jeznel Raeaih Jaikeis Josetyna Jawy\n",
      "Batch Loss:43.76099395751953 at iter: 90000\n",
      "Batch Loss:42.776180267333984 at iter: 91000\n",
      "Batch Loss:57.78956604003906 at iter: 92000\n",
      "Batch Loss:51.27497482299805 at iter: 93000\n",
      "Batch Loss:54.764495849609375 at iter: 94000\n",
      "Batch Loss:40.87724304199219 at iter: 95000\n",
      "Batch Loss:52.925193786621094 at iter: 96000\n",
      "Batch Loss:45.47420120239258 at iter: 97000\n",
      "Batch Loss:51.73523712158203 at iter: 98000\n",
      "Batch Loss:46.57704162597656 at iter: 99000\n",
      "Running Avg Loss:2.0234821840700694 at epoch: 2\n",
      " Kailynd Khris Kaleni Khana Kria Kayle Kyana Kine Kamaiah Kala Kennitha Kyneysh Kesilinn Kaelyn Kelle Khalisy Kasiyne Kevelate Khimie Kynn Kyeera Keldari Kallya Kessei Kynivi Khils Kaelyn Krian Kilyn \n",
      "Batch Loss:45.493831634521484 at iter: 100000\n",
      "Batch Loss:45.758426666259766 at iter: 101000\n",
      "Batch Loss:48.838924407958984 at iter: 102000\n",
      "Batch Loss:46.77040481567383 at iter: 103000\n",
      "Batch Loss:54.82847213745117 at iter: 104000\n",
      "Batch Loss:69.82408905029297 at iter: 105000\n",
      "Batch Loss:38.39285659790039 at iter: 106000\n",
      "Batch Loss:44.279579162597656 at iter: 107000\n",
      "Batch Loss:49.71200180053711 at iter: 108000\n",
      "Batch Loss:43.32918167114258 at iter: 109000\n",
      "Running Avg Loss:2.0290001581916584 at epoch: 2\n",
      "Fraccheh Danzei Emana Equan Fari Emmir Empreteen ECnialla Emmry Chren Emerile Kelynna Eliserie Kbissasy Greethan Britzee Erli Gumarah Denalai Evanis Meelallia Ehmaron Emone Erleah Ennal Hallendi Frace\n",
      "Batch Loss:52.86603546142578 at iter: 110000\n",
      "Batch Loss:55.593482971191406 at iter: 111000\n",
      "Batch Loss:44.65518569946289 at iter: 112000\n",
      "Batch Loss:40.209346771240234 at iter: 113000\n",
      "Batch Loss:35.68409729003906 at iter: 114000\n",
      "Batch Loss:47.422306060791016 at iter: 115000\n",
      "Batch Loss:62.064449310302734 at iter: 116000\n",
      "Batch Loss:58.40168380737305 at iter: 117000\n",
      "Batch Loss:47.80767822265625 at iter: 118000\n",
      "Batch Loss:50.33867645263672 at iter: 119000\n",
      "Running Avg Loss:2.051120697789639 at epoch: 2\n",
      "itharin Aedina Addilae Azalia Aaleigeeh Avealyse Aeviua Zeehat Avelia Aaryah Zanila Aneseicen Adniyasciyea Zarliena Adilliela Aeliadba Yarki Atezlaina AfwyannZeannn Aevy Yabri Zuika Zaribesh Alyzm Bri\n",
      "Batch Loss:47.880287170410156 at iter: 120000\n",
      "Batch Loss:49.15300750732422 at iter: 121000\n",
      "Batch Loss:50.834476470947266 at iter: 122000\n",
      "Batch Loss:42.64279556274414 at iter: 123000\n",
      "Batch Loss:47.27515411376953 at iter: 124000\n",
      "Batch Loss:55.167484283447266 at iter: 125000\n",
      "Batch Loss:57.978939056396484 at iter: 126000\n",
      "Batch Loss:60.89590072631836 at iter: 127000\n",
      "Batch Loss:39.9307746887207 at iter: 128000\n",
      "Batch Loss:46.702049255371094 at iter: 129000\n",
      "Running Avg Loss:1.9633595923100597 at epoch: 2\n",
      "atee Kaidy Keevia Kitree KinniKo Jesen Kemzari Kayalla Koian Kille Keithee Kahgia Kazlyah Kenaja Keerah Kanariri Kavahmalynne Kiunl Keiam Jusa me lynne Kena Kabsrila Keelannee Kammaly Kamceya Kerym Ke\n",
      "Batch Loss:48.07280731201172 at iter: 130000\n",
      "Batch Loss:37.3900260925293 at iter: 131000\n",
      "Batch Loss:46.349430084228516 at iter: 132000\n",
      "Batch Loss:43.522823333740234 at iter: 133000\n",
      "Batch Loss:44.99625778198242 at iter: 134000\n",
      "Batch Loss:51.37821578979492 at iter: 135000\n",
      "Batch Loss:59.577110290527344 at iter: 136000\n",
      "Batch Loss:69.89051818847656 at iter: 137000\n",
      "Batch Loss:59.09746551513672 at iter: 138000\n",
      "Batch Loss:48.868988037109375 at iter: 139000\n",
      "Running Avg Loss:2.0512164732195437 at epoch: 2\n",
      "e Cindlya Walanybbittna SNatari Coprahdr S Yehencon Cviyah Zaceela Vilshah Dennie Zozolyn Chadary Yatina Zimolyna Zyneydi Ziani Tabbei Emarna Yona Rathyrisa Meeliz Vinlow Yah Sarbri Zoriah Bineladi Y \n",
      "Batch Loss:53.69212341308594 at iter: 140000\n",
      "Batch Loss:51.3989372253418 at iter: 141000\n",
      "Batch Loss:56.827003479003906 at iter: 142000\n",
      "Batch Loss:49.01552963256836 at iter: 143000\n",
      "Batch Loss:58.13405227661133 at iter: 144000\n",
      "Batch Loss:55.97218704223633 at iter: 145000\n",
      "Batch Loss:53.30255126953125 at iter: 146000\n",
      "Batch Loss:47.240299224853516 at iter: 147000\n",
      "Batch Loss:61.50542449951172 at iter: 148000\n",
      "Batch Loss:62.925933837890625 at iter: 149000\n",
      "Running Avg Loss:2.2149688709621316 at epoch: 2\n",
      "s Koer Jenlion Coslee Ken Lissonneote prickyn Jtosh Lodgen Enildah Ganan Rouk Kod Pryn Mashann Cly Taximanaw Yantie Timay Bemyn Derean Triaph Jovon Mal Brngen Maxxamarl Laivickan Daparil Kaolen Yraido\n",
      "Batch Loss:52.61015701293945 at iter: 150000\n",
      "Batch Loss:54.663578033447266 at iter: 151000\n",
      "Batch Loss:50.60238265991211 at iter: 152000\n",
      "Batch Loss:55.708133697509766 at iter: 153000\n",
      "Batch Loss:55.033973693847656 at iter: 154000\n",
      "Batch Loss:69.12718200683594 at iter: 155000\n",
      "Batch Loss:57.14566421508789 at iter: 156000\n",
      "Batch Loss:62.05242156982422 at iter: 157000\n",
      "Batch Loss:61.75387191772461 at iter: 158000\n",
      "Batch Loss:54.309242248535156 at iter: 159000\n",
      "Running Avg Loss:2.173553887228668 at epoch: 2\n",
      "yon Adrven Dhainz Haid Ceamnt Jarveny Cersin Esyon Kocel Haronn Maylyn ZamanronKeya Malon Kani Prishan Morree Ppenia Ibell Karden Karyras Keven Makymarghm Nawaishe Rinziy Lamay Jander Gzano Zekidyr Zo\n",
      "Batch Loss:50.13243865966797 at iter: 160000\n",
      "Batch Loss:46.43528366088867 at iter: 161000\n",
      "Batch Loss:51.69562911987305 at iter: 162000\n",
      "Batch Loss:57.0046501159668 at iter: 163000\n",
      "Batch Loss:58.43838882446289 at iter: 164000\n",
      "Batch Loss:49.161842346191406 at iter: 165000\n",
      "Batch Loss:58.23976516723633 at iter: 166000\n",
      "Batch Loss:47.42837142944336 at iter: 167000\n",
      "Batch Loss:53.45949172973633 at iter: 168000\n",
      "Batch Loss:66.42975616455078 at iter: 169000\n",
      "Running Avg Loss:2.1569094023827464 at epoch: 2\n",
      " Shasin Taanneig Zorante Danneelaz Lerkyn Kyrezon Keyten Kablin Lalel Misody Khones Kwydauv Korsir Llycay Keear LorvingOn Chianny Kyamerin Kaizle Kilee Jaheem Jisti J Lagimire Jur Kouson GaziellMeon K\n",
      "Batch Loss:55.512001037597656 at iter: 170000\n",
      "Batch Loss:58.175048828125 at iter: 171000\n",
      "Batch Loss:49.40916061401367 at iter: 172000\n",
      "Batch Loss:57.73189926147461 at iter: 173000\n",
      "Batch Loss:45.89181900024414 at iter: 174000\n",
      "Batch Loss:54.71700668334961 at iter: 175000\n",
      "Batch Loss:41.29224395751953 at iter: 176000\n",
      "Batch Loss:49.760807037353516 at iter: 177000\n",
      "Batch Loss:61.55522537231445 at iter: 178000\n",
      "Batch Loss:52.664642333984375 at iter: 179000\n",
      "Running Avg Loss:2.187742201010138 at epoch: 2\n",
      "l Tentwene Linne Kloid MirleliM Yte Lelan Koyavon Kyndro Manso Frikan Kaiam Meckk Gusin Kyyatruan Keandrauge Johad Macht Lay Tanied Farm Kallin Kend Mishaw Mademey Yuber Jaucon Jaulli Mirni Mordif Mat\n",
      "Batch Loss:56.36181640625 at iter: 180000\n",
      "Batch Loss:62.527587890625 at iter: 181000\n",
      "Batch Loss:67.96092987060547 at iter: 182000\n",
      "Batch Loss:55.89035415649414 at iter: 183000\n",
      "Batch Loss:42.80672836303711 at iter: 184000\n",
      "Batch Loss:53.409645080566406 at iter: 185000\n",
      "Batch Loss:61.49973678588867 at iter: 186000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-c708541e3595>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_chars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#loss_batch.backward(retain_graph=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mloss_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Batch Loss:{loss_batch} at iter: {len(losses)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python\\Anaconda\\envs\\py36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python\\Anaconda\\envs\\py36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    losses = []    \n",
    "    for batch in get_batch(X_train, y_train, seq_length):   \n",
    "        X_batch, y_batch = batch        \n",
    "        optimizer.zero_grad()   \n",
    "        h_prev = rnn.init_h()\n",
    "        loss_batch = 0\n",
    "        for ch_id in range(len(X_batch)):\n",
    "            y_score, h_prev = rnn(X_batch[ch_id], h_prev)            \n",
    "            loss = loss_fn(y_score.view(1,-1), y_batch[ch_id].view(1))\n",
    "            loss_batch += loss\n",
    "            losses.append(loss.item())                    \n",
    "            \n",
    "\n",
    "            if len(losses)%10000==0:\n",
    "                print(f'Running Avg Loss:{np.mean(losses[-10000:])} at epoch: {epoch}')  \n",
    "                print(sample_chars(X_batch[ch_id], h_prev, 200))\n",
    "        #loss_batch.backward(retain_graph=True)\n",
    "        loss_batch.backward()\n",
    "        if len(losses)%1000==0:\n",
    "            print(f'Batch Loss:{loss_batch} at iter: {len(losses)}')  \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h_prev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2857daaac482>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'h_prev' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(h_prev.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d32f33b7f603>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_batch' is not defined"
     ]
    }
   ],
   "source": [
    "batch = get_batch(X_train, y_train, seq_length).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2f938aba0892>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_batch' is not defined"
     ]
    }
   ],
   "source": [
    "X_seed = X_batch[ch_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " deand I dron. Thinte all now, weill peoth tile nhelnd de afy grree gop lome fale the ghe to fen see thaende fuv.\n",
      "trewerpe eorl do Iof fe lerss.\n",
      "Sheymeashas ushp vory redy thr in.\n",
      "I gops ot ge I Ie thes gone gim wevh ge in dig sadte hing venday melshat wad jbe gasile the osre dedibig ens ind than si kanth and a ledt rec ca to lon hing ind Id tily eveathe le ceoi ge nopal ope peeng.\n",
      "Anie dedsof whes dei keled Ishane,reess demen that the someshcppin thae toult.\n",
      "Anet he ge gocpopled I gop.\n",
      "I oull ber beond wipe yomttresve, tupre toule her died Is o at old It thany Sgos haver thrySe the inag bidte I sa  Iveae gor.\n",
      "Ired reter fing lalt noige thantintian emo wol an It sus wha evy chat he nhad igry necp k.\n",
      "\"ind geantr at doaut maledl whanr ofeo lid lectpeeshe she I at snd precand oud boly ibed h teln youd I dow.\n",
      "I they calerton pom aple.\n",
      "in. I I wo touse soo kike bey the looma ruys gad.\n",
      "Bore goded tomnn dou kan gyhe .\n",
      "Yowes dot tits Ir cobend picre gous. drend herl giak onot shousitentid inewe Anes aid sell ad. Ance  hasve wey ats dutin wind miave imes lleide inekid cauting dedisite gelle to tit Oele were.. Sat wece, pAt feny reaptta fhape thees thac be lile. I wend ovh to tout acgoug mpaly at dn whas I thesdts in. Thas hif didnd anere dit woum atd anle.\n",
      "Thiut peeninss.\n",
      "I thas.\n",
      " Iledop gout ans, cout wis houlled hat tere it weloullere sert bey have gere.\n",
      "eet vind.\n",
      "I ke dop he thatede I inegl cou waem peowe fay to leve.\n",
      "So wiang sac sbecmnd Ie theve.y lmateemd dhant thech bed snintop bid kelcosesstrs is bal triagihe the be kapeon weel. mey fhed heyreere simibe shheys won wo val diff frenl toum it yo hevitiat angind.\n",
      "SomN  It at hrech tit.\n",
      "And ary.\" nhe wa bad keve se.\n",
      "I\n",
      "Ave sopd. nd Wave uple ve run at tinigale thaymedweont havte rautbant i cow an ant.\n",
      "Ad bentrey thesend ang wi reys wouone.\n",
      "I lend her Is ruckeon I cand a dodic.\n",
      "eve mem thig ang tos now cente ous penct an.\n",
      "At ticanad und the sa biad an vat tiy up. Is hrend hawy A topre shave\n",
      " I woled men all se.\n",
      "Beor peno thott fopse haved teverept as hipid poc beogind vho want seedtt.\n",
      "Al Ing ane bidteny te, wellcet.\n",
      "I\n",
      "Thiat ar kave and ive se aintangasg rponnmsI ha. To tun game Ive Iv?ilast omey hone meal hid dobe ise nes cofoa whe siglery s, touybevagtecalt madsr.\n",
      "Bung ild oud o ant?k ring yot sape so Im hide the shele we het tit Sant ins af beoncs dec, veakingem in o frulep ruedeesoncs me noop covo be a..\n",
      "Au beant Ialed sple wide golk re ren apll terw is arlope lednd geve go It hing yopk. I whe licot pebest due belA the mead ang swo ubeng erith  I abat row\n",
      "Ann te peom cow vendn, hes tome fianinne ond hala hhhe sir, Avat gaybok os sthe thatveders yoter thang mutk.\n",
      "Sopare ong teotiss sinn shel meoweeve so dils so dele ract hey.\n",
      "Buy an keath ta ase re al in tiny who me. I womeat erusky I I werenbe so want in herl rung.\n",
      "Ang they git hamliss oud Ive the sover thettd Is nhupe leng ucdig eno doulng rua toave thiou any vo.\n",
      "\n",
      " ay I vraid instet pilleaiglunbiny se peoid we lender. ne cok nobeotess be bew I sHuy Iny shere theytre pere ynou dowat wer keach \"eu.\n",
      "I thes Id a ofw we theare tits havat Touts to wa Ive pare weve, minsa feeyn hes that wamand.\n",
      "Id sant thal we bofk Id o de nec ayeelgound tBund bad deo, thay Iming yo dnond.\n",
      "Buttine eved has int he goct winthe I we ancg cowepe reat thgouv, hrmedeat thave ave we we lemnich tout aver that wet tamest \"ad ho I wofyry thate pevadin Is bede. Ivefid dwowa taunt thet trly hielond hinte supesour shat kaonuvere the ugolg bell d pald. I an of ipre boung wemre gop. I ther thee hapid toofrssey belthelit sine of ange nutil rue whag tpeung you geere thot cind thdagidid demckbey tol pnoustothe p and \n",
      "roparung.\n",
      "Sand be anb sunan they Im racc, those sere sowede no nowe. I an ca, oude eesd oudecared.\n",
      "\n",
      "\n",
      "ra ung rent wa fuve sond s rerigth.\n",
      "on nit wan wicet. Ios suwd tunit gevat sadt ane thal waot, det thect wape tiileobNs, mon damy the tish.\n",
      "I I ave ro.\n",
      "y\n",
      "Ang ave con nhat sere pet. Sl hts tiey yo tidon. Ane pheith re you noutttyn terl ges Gate Ioud  Isered.\n",
      "ee dod bell so pa wundy mand s ot nhas.\n",
      "Ar hak cia le on. Ire keet she eor geck osth get drobu, thamoly seo sean the wesnecals ow\n",
      "I thim dar pauley. Iow soplalkef be shey frow thats pa tu teoy har wha euvar, seprottse todt hruch so sing drodd Be wowe thes, sarl ta deve ulley tia, sead yo Is mfo kewe thid thetopleut son Ierat youdegh that aapn  I mol we I ged weool pouns.\n",
      "Ave tou so ta whet woung, eoid ow. Itin.\"\" ap.\n",
      "Bude.\n",
      "Thewensit aket dewithered pall bopend se eode.\n",
      "Al ter knot cocont heg thavel lend hatrrey sell alk yrauy.\n",
      "Buyt Feend ung  I ma yo pin degrd havere gons sulle he r oud ank have tha empseed the gopln y\n",
      "Ba und did breontpid ang farides astioas wemllit.\n",
      "No veous.\n",
      "ane  hed kand heps. I the yoces? Ie un do I\n",
      ".\n",
      "ale this jwyf he lean teme. We the sha wo beI maece suond anyton muteng dous doe haveds srok.\n",
      "I thered fmeas thonge. I sans pel bes peemest she to sny gooplleralcaot in. BTas tidinse touce sicout net this didl faoul sa fore at ind Ierd Theme lroustt geche swo sin o pet dodf madybe go toumy and Ie. Wting. The yoplde Thotm noole Sidnt mphop be ikeandis There peothe win thave y\n",
      "A mageng thedy anaged has weve rerp, ho paangt? Iver inntr I feray a d hausso goag santher sno tor domens ave mens the youd yrut\n",
      "n. I depeets dat sal dat han cama I sond de thiled hat the rakoustreaoud o wedn af wats tar I he so Owheing.\n",
      "I enfens renabeapprhat.\n",
      "seThe glit as? Tune they nhe dent chat thang  Bac hopit seseatd I c, deeade ily weploely theat rendey toing, the ko oud.\n",
      "Th, nich, wovand haf hrrang. The hin wes Pend sevet  trin poo It geidl I.\n",
      "Ave wat be dad hits phickn co me bit cend nas buellidite Ir  in has.\n",
      "Bouveoud cant is n uns taliald I vele shat Danbo I wButhamape rene hereck nild yo, Ive eve it.\n",
      "Ir welg the dere  ca we ce pom. I ave amel weenlpe fac tho have rum wan sheg timams, douctr pat the\n",
      "yoa fac co ka causiav eong ged Ir cno?ondevacisgr, det  We te.\n",
      "und Trot ise, I spat.ndme bika tind therd me lhavee.\n",
      "Bore dto whoud.\n",
      "H tou go bit tidpeseiske dit ast beera.\n",
      "I teral I ennd so wo ras. lesher gedy tely se, don Seve and aned thanouma thetce.\n",
      "eo if peounk Ih pore Ive ldott is popleth. It avecito y wechears eadlich to ho uckager ugh sis os rhe anouy int Ir dens anc, sas \n",
      "nort dycaste goy I at that ibed thoe wur, have cand har tisiniosey lopre ckney thesbreg anecn cnn an dessrnd theo roll beeghr soalshere cuent p ang.\n",
      "BWecel knrot fo sasticant riwk Ousent, \n",
      "Amt thinet In,srde fre whes kige nhitt nite he ther I dece, herletsas Man fit sing irok.\n",
      "\"Souse of casa tialing dinten n beth ,er I duauc wthistied Isos al iin ort raalkna dape chic hott piglert Itode tho tos ane tellle of ipoung jut ave. In hape asthe, IdI de ineg al wovegna afas, wo his her hetre dect the leoult teod yo win toto anavait they te oneg ind tome whiel so Id.\n",
      "\"So shat bed I pacd an wey aled keyst tom recigemwe.\n",
      "Shiing of nont bevedbou plithe had.\n",
      "That.\n",
      "I I rbus yut wan keng cotan, I heer she nond. ISe thiiads ruve wige ay hap he co hauy eopplaoul ter oulke thecs seedbent have an.\n",
      "Anis be, binn ang bheerpolet wed, It che.w\n",
      "mey ing teed tho hel sempel . Blaved sico fom gere.\n",
      "re ledy I youveont hat cef nedhil didtdell rell dew Iey Sad that sipe gouy anot ded, know I kluat thed ane mundth.\n",
      "A gout kant a poy sot heik yNoy me.\n",
      "As paot aeve  de evet be youd Iny oust. I theos ond demakndthica, men gemn, vanth s1eorsnis pine ay he  ow a0 i knant denlk Sadt ine anding wBnotered  Al en. I thit an,e thral, obows I thith ruts shenbt aond keoud. Iowe mpwhe tolthay an medes mentel nhat save. S. gout he eopere.\n",
      "I cousre.\n",
      "cond I have reve tmriclrcunt. I sot the reld I oist waly bopleg pik an tuvopon the weadme.sBeve dove \"Sra tou. Id than,  Helt we gap, sam rult I wo beig in now an I.\n",
      "Ant. Tun dout. me, an ut bary thot wapleanl lots shes Isougereade the theyreg didnst ored mive theam lilk ted. He heit. Thrin. geng.\n",
      "Wer bi dous eve ham to al  of eing teaw the no weath founn ols. Bdout aveget disso tryo gaderism.\n",
      "Abeent dupeansy wan that  deer peclls o dede ce ofey thasst wrel, am ca po onng.\n",
      "I rie wesing gedouth s, bi wind. \n",
      "eme thhas mronw hee,ulrars. Is poniathtcapme.\n",
      "A wiem. Ave o dounke geed maved Iver kealieg caant \"at aoulg. gelligst thatM ma the ou kres nuch, Ave ret ate se pours hat wicane ahte utprrot, be chel  int I hat Wer sonn his bealge ave ound. I\n",
      " Toui  me It rud I lensabe ho is nntth dee noth mo reke yond has wI see yo klouyt Irtd s dow h pid gody whappy Inv  nobuc,. With mod.\n",
      "I enon bout?rop. s.\n",
      "Alet.\n",
      "SWe an.\n",
      "Abiout yNow woi lecka gat they o bed I cnat thou thoond s aionthtmre tout, I frel paalmt toobettppand Jnos so ta wond \"ourhe yor Ses ane she roume to het raved ds yriga.\n",
      "I she kecte te dos bnoully Nin a kyoter thing abe buoknd I keoe soelchiy I we kfoll dnge lech knnt aatull tote che sore ve eunt tha.\n",
      "At taout do shat thang endt ans mill c ca ine, they ats ley und rid kerec ouse ad hetne Inatreyt af poplle? I dito atute in prot so dededs sarlise, wi Pa coy Thimedo gen atioleth knst sind ylet and eatp ol ko gos hi rend lecs co toe.\n",
      "thast keot go ang I hat.\n",
      "I I woveie kat sed geing onet thayse sa lecn.\n",
      "OOpeouts talontt red Funat docall vere.\n",
      "o gishye sis on thac dave. se lis sorrit ind I Ich he kare you I ald hesy I theast fule.\n",
      "\"Sremet soud be donsit and maple.\n",
      "I ruow ants I am, avet. nhice ice at aad a nelld dictey an to faabemping des.\n",
      "I Iorsy what wevet geos poppsh sekve  whe lcaitheong it the thi\n",
      "int we hive ruyt inedi. duy. I avell inegst wedrer the they and wave the wiss wedrotr athandrathandad \n",
      "mandte.\n",
      "re dicans aguy snhade wo mimndy An. I ag.\n",
      "nond te lo Miot weadtang thctheysf I ghat.\n",
      "I\n",
      "Ishy ata eitin. Ive soll  ound. Adl baw hor soid drom os, \n",
      "BToume olle simdiout best.\n",
      ".\n",
      "Shey nad beare.Osst be I wing dingopt dikn, I heded han. I we.\n",
      "ant. Soup thas heverid.\n",
      "AHed.\n",
      "Irbeve to keow mond I doat, ne aty int now to f tinid yo shaplabseicere thaaknps cowe Som\n"
     ]
    }
   ],
   "source": [
    "print(sample_chars(X_seed, rnn.init_h(), 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 53)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Whh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Why.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs[t] = np.zeros((vocab_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs[t][inputs[t]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(Wxh, xs[t]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys[t] = np.dot(Why, hs[t]) + by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[t].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.3183783825123"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(1.0/3000)*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hprev = hs[t-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ikama enaliia Janden Celen Badkia Darine Jumah Jagkida Asmyanat Caima Ediouy Inriserya Janieghebe Jame Giyne Jozabkatgh Alance Jomvi Jorielya Eyah Aillana Kateliuh Babye Makjan Calisse Tynnibele Jaigz'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(ix_to_char[ix] for ix in sample(hprev, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed_ix = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = hprev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 53)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(Wxh, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 53)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.25729783880305"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(vocab_size)*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hprev = np.zeros((hidden_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emma Olivia Sophia Isabel'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[p:p+seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mma Olivia Sophia Isabell'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[p+1:p+seq_length+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " PfKTwPfmTLjbeVZCLEYRbGltieocPNNtsnTWVZNjehJnZLQRvvrMFQNpxCxHQncWpvjBYZYqasQfIdjPqtrPTEYDcCOxngJamYZCVpjyxVEIQRCbvbqpYtPM ZNVXcNiIDRixwFDVnkWUscUzu juwCkYuBqHeUVcyiWELKcoIr nr TKNCDpxLGyyKENSvJnmvweRQQ \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0398582039920804"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[t][targets[t]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.22462687e-02],\n",
       "       [3.70884873e-04],\n",
       "       [1.47718282e-01],\n",
       "       [4.91778089e-04],\n",
       "       [1.43579039e-02],\n",
       "       [1.94613594e-03],\n",
       "       [3.98582040e-02],\n",
       "       [1.83239784e-03],\n",
       "       [1.40377295e-03],\n",
       "       [2.25524356e-03],\n",
       "       [1.86338027e-02],\n",
       "       [1.82163330e-03],\n",
       "       [1.36034842e-03],\n",
       "       [5.34832575e-03],\n",
       "       [5.31882994e-03],\n",
       "       [2.56723943e-02],\n",
       "       [2.08954666e-01],\n",
       "       [1.67814105e-04],\n",
       "       [5.43953948e-02],\n",
       "       [1.23416029e-02],\n",
       "       [2.71522941e-02],\n",
       "       [7.07506732e-04],\n",
       "       [3.90479874e-03],\n",
       "       [3.07619937e-03],\n",
       "       [6.03594339e-04],\n",
       "       [2.60599790e-02],\n",
       "       [3.70108202e-05],\n",
       "       [2.20467155e-03],\n",
       "       [8.62683131e-03],\n",
       "       [6.98119822e-04],\n",
       "       [5.34681858e-03],\n",
       "       [2.66708994e-03],\n",
       "       [1.11460252e-02],\n",
       "       [1.62131224e-03],\n",
       "       [4.59798728e-03],\n",
       "       [2.41688939e-03],\n",
       "       [7.15391514e-04],\n",
       "       [1.00846412e-03],\n",
       "       [7.64864017e-04],\n",
       "       [1.19025338e-02],\n",
       "       [4.88808782e-02],\n",
       "       [6.65579358e-02],\n",
       "       [1.07478080e-03],\n",
       "       [6.73335329e-04],\n",
       "       [2.65998096e-03],\n",
       "       [3.20836611e-02],\n",
       "       [2.08930948e-02],\n",
       "       [3.64345458e-04],\n",
       "       [1.19719124e-01],\n",
       "       [6.08244844e-04],\n",
       "       [5.51525083e-04],\n",
       "       [4.71561737e-03],\n",
       "       [2.94634093e-02]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[t].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11848706, -0.05649749,  0.02691294,  0.19014995,  0.07061441,\n",
       "       -1.16252144,  0.12697324, -0.08889963, -0.06267175, -0.40105323,\n",
       "       -0.18747102, -0.18140617,  0.07432117,  0.34334437,  0.06726984,\n",
       "        0.20165935,  0.32747234,  0.04495337,  0.1142929 , -0.05521551,\n",
       "        0.00885307,  0.33516984,  0.13067932, -0.10904185,  0.19814699,\n",
       "        0.0913149 , -0.06236525, -0.01253117,  0.41185781, -0.3912567 ,\n",
       "       -0.1494085 ,  0.36435163, -0.44824475, -0.09278199,  0.34402929,\n",
       "       -0.15928979, -0.26921809,  0.05811367, -0.13787257,  0.20121986,\n",
       "        0.01388096,  0.07998907,  0.05387593,  0.0689815 ,  0.26390591,\n",
       "        0.0065182 , -0.09641561,  0.20798219,  0.07988806,  0.20825779,\n",
       "       -0.26811509, -0.03696904, -0.16780612, -0.38446364,  0.28463213,\n",
       "        0.14157797, -0.19357392, -0.16630077,  0.11378831,  0.25725707,\n",
       "        0.17818427,  0.008875  , -0.06520907,  0.12799508,  0.22632408,\n",
       "       -0.17792747,  0.19119546, -0.04671774, -0.45145593,  0.05777601,\n",
       "       -0.09830163, -0.31653179,  0.15220932,  0.20794963, -0.03008459,\n",
       "       -0.26965372,  0.1204601 ,  0.34252047,  0.09401755,  0.52394405,\n",
       "       -1.28136092, -2.30766055,  0.0621971 ,  0.65735221, -0.06726325,\n",
       "       -0.10967955,  0.37269327, -0.12546259,  0.03567711, -0.12430217,\n",
       "       -0.11907313, -0.0464631 ,  0.29016598,  0.23329175,  0.22403798,\n",
       "        0.04957504, -0.03167637, -0.15892867, -0.21109618, -0.03625879])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
